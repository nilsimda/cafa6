from Bio import SeqIO
import polars as pl

def create_test_df(fasta_path: str = "dataset/Test/testsuperset.fasta") -> pl.DataFrame:
    """Create a dataframe with the protein ids, sequences and taxon_ids from the test superset fasta file."""
    data = [
        (*record.description.split(" ", 1), str(record.seq))
        for record in SeqIO.parse(fasta_path, "fasta")
    ]

    return pl.DataFrame(
        data,
        schema=[
            ("protein_id", pl.Utf8),
            ("taxon_id", pl.Int64),
            ("sequence", pl.Utf8),
        ],
        orient="row"
    )

def create_train_df(fasta_path: str = "dataset/Train/train_sequences.fasta", taxon_file: str = "dataset/Train/train_taxonomy.tsv") -> pl.DataFrame:
    """Create a dataframe from the train fasta file.

    Columns:
        - db: The database the protein was found in (always sp).
        - protein_id: The uniprot accession id.
        - gene_name: The name of the gene the protein is associated with.
        - sequence: The amino acid sequence of the protein.
        - taxon_id: The taxon id of the organism the protein is associated with.
    """
    data = [
        (*record.id.split("|", 2), str(record.seq))
        for record in SeqIO.parse(fasta_path, "fasta")
    ]

    train_df = pl.DataFrame(
        data, 
        schema=["db", "protein_id", "gene_name", "sequence"],
        orient="row"
    )

    train_taxons_df = pl.read_csv(taxon_file, separator="\t", has_header=False).rename({"column_1": "protein_id", "column_2": "taxon_id"})

    return train_df.join(train_taxons_df, on="protein_id")

def onehot_encode_taxon_ids(
    df: pl.DataFrame,
    k: int = 77,
    ref_df: pl.DataFrame = create_test_df(),
    phylum_map_path: str = "dataset/taxon_phylum.tsv"
) -> pl.DataFrame:
    """
    Encode taxon_ids as a single list column (one-hot vector).
    
    Features:
    - Uses top-k IDs and Phyla from ref_df to build a fixed vocabulary.
    - Returns a deterministic vector (sorted by vocabulary category).
    - Returns the dataframe with a new column 'taxon_ohe'.
    """
    
    # --- 1. Load Map & Prepare Keys ---
    phylum_map = pl.read_csv(phylum_map_path, separator="\t", has_header=False).rename({"column_1": "taxon_id", "column_2": "phylum"})
    
    # Ensure consistent types for joining
    phylum_clean = phylum_map.with_columns(pl.col("taxon_id").cast(pl.Int64))
    
    # --- 2. Learn Vocabulary from Reference (ref_df) ---
    # Get Top K IDs
    top_ids = (
        ref_df.group_by("taxon_id")
        .len()
        .sort("len", descending=True)
        .head(k)
        .get_column("taxon_id")
        .to_list()
    )
    
    def get_categories(input_df):
        """Helper to map ID -> Category based on Top K and Phylum."""
        return (
            input_df.with_columns(pl.col("taxon_id").cast(pl.Int64))
            .join(phylum_clean, on="taxon_id", how="left")
            .with_columns(
                pl.when(pl.col("taxon_id").is_in(top_ids))
                .then(pl.col("taxon_id").cast(pl.String))
                .otherwise(pl.col("phylum").fill_null("Other"))
                .alias("taxon_cat")
            )
        )

    # Determine the fixed sorted vocabulary from reference data
    ref_categorized = get_categories(ref_df)
    vocab = sorted(ref_categorized.get_column("taxon_cat").unique().to_list())
    
    # Ensure 'Other' is in vocabulary if not present, to handle unknowns
    if "Other" not in vocab:
        vocab.append("Other")
        vocab.sort()

    # --- 3. Apply to Input DF & Vectorize ---
    df_cat = get_categories(df)
    
    # Force input categories to adhere to vocabulary (map unknowns to 'Other')
    df_aligned = df_cat.with_columns(
        pl.when(pl.col("taxon_cat").is_in(vocab))
        .then(pl.col("taxon_cat"))
        .otherwise(pl.lit("Other"))
        .alias("taxon_cat")
    )
    
    # Generate Dummies
    # We explicitly specify one dummies call to generate columns
    df_dummies = df_aligned.to_dummies("taxon_cat")
    
    # Columns generated by to_dummies look like "taxon_cat_Value"
    # We need to ensure every vocabulary item has a corresponding column
    dummy_cols_map = {v: f"taxon_cat_{v}" for v in vocab}
    existing_cols = set(df_dummies.columns)
    
    # Create expressions for the final vector
    # If the column exists, use it; otherwise create a literal 0 series
    vector_exprs = []
    for v in vocab:
        col_name = dummy_cols_map[v]
        if col_name in existing_cols:
            vector_exprs.append(pl.col(col_name))
        else:
            vector_exprs.append(pl.lit(0, dtype=pl.Int8))
            
    # Compress all dummy columns into one list column and cleanup
    return df_dummies.with_columns(
        pl.concat_list(vector_exprs).alias("taxon_ohe")
    ).drop([c for c in existing_cols if c.startswith("taxon_cat_")])
